
\chapter{Python Modules}

% -------------------------------------------
\section{K-fold Nested Cross-Validation}\label{appendix:nestedcv}

\begin{minted}[mathescape,
               linenos,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}

  class NestedCV:
     def __init__(self, dataset, model, param_grid, outer_K=5, inner_K=4):
        self.X = dataset.drop(columns='y').to_numpy()
        self.y = dataset['y'].to_numpy()
        self.n_samples, self.n_features = self.X.shape
        self.model = model
        self.param_grid = [dict(zip(keys, p)) for keys, values in 
            [zip(*param_grid.items())] for p in product(*values)]
        self.outer_K = outer_K
        self.inner_K = inner_K

     def _zero_one_loss(self, y_true, y_pred):
        return np.sum(y_true != y_pred)

     def _compute_metrics(self, y_true, y_pred):
        TP = np.sum((y_true == 1) & (y_pred == 1))
        TN = np.sum((y_true == -1) & (y_pred == -1))
        FP = np.sum((y_true == -1) & (y_pred == 1))
        FN = np.sum((y_true == 1) & (y_pred == -1))

        accuracy = (TP + TN) / (TP + TN + FP + FN)
        precision = TP / (TP + FP) if (TP + FP) > 0 else 1.0
        recall = TP / (TP + FN) if (TP + FN) > 0 else 1.0
        return accuracy, precision, recall

     def _KFold(self, K, n_samples, indices=None):
        if indices is None: indices = np.arange(n_samples)
        np.random.shuffle(indices)
        split_points = np.linspace(start=0, stop=n_samples, num=K+1, dtype=int)
        test_indices = [indices[split_points[i]:split_points[i+1]]
            for i in range(K)]
        train_indices = [np.setdiff1d(indices, test_idx)
            for test_idx in test_indices]
        return zip(train_indices, test_indices)

     def train(self):
        # Outer CV, evaluating the best hyperparameter combination 
        self.training_results, self.test_results = list(), list()
        for fold, (outer_train_indices, test_indices) in
            enumerate(self._KFold(K=self.outer_K, n_samples=self.n_samples)):

            # Inner CV, finding the best hyperparameter combination
            print(f'Fold {fold+1}')
            min_loss = np.inf
            best_params = None
            for params in self.param_grid:
                losses = list()
                for train_indices, validation_indices in self._KFold(
                        K=self.inner_K,
                        n_samples=len(outer_train_indices), 
                        indices=outer_train_indices
                    ):
                    model = self.model(**params)
                    model.train(self.X[train_indices], self.y[train_indices])
                    y_pred = model.predict(self.X[validation_indices])
                    loss = self._zero_one_loss(self.y[validation_indices], y_pred)
                    losses.append(loss)

                avg_loss = np.mean(losses)
                print(f'Hyperparameter combination: {params}', 
                    f'Average Loss: {avg_loss}')
                if avg_loss < min_loss:
                    min_loss = avg_loss
                    best_params = params

            print(f'Retraining the model with the best hyperparameter combination:
                {best_params}')
            model = self.model(**best_params)
            model.train(self.X[outer_train_indices], self.y[outer_train_indices])

            # Training error
            y_pred_train = model.predict(self.X[outer_train_indices])
            accuracy, precision, recall = 
                self._compute_metrics(self.y[outer_train_indices], y_pred_train)
            self.training_results.append([accuracy, precision, recall])
            print(f'Training Set Evaluation: Accuracy {accuracy:.2%}', 
                f'Precision {precision:.2%}, Recall {recall:.2%}')

            # Test error
            y_pred_test = model.predict(self.X[test_indices])
            accuracy, precision, recall = 
                self._compute_metrics(self.y[test_indices], y_pred_test)
            self.test_results.append([accuracy, precision, recall])
            print(f'Test Set Evaluation: Accuracy {accuracy:.2%}, 
                f'Precision {precision:.2%}, Recall {recall:.2%}')
            print('-'*80)

        # Average results
        accuracy, precision, recall = np.mean(self.training_results, axis=0)
        print(f'Average Training Results: Accuracy {accuracy:.2%}, 
            f'Precision {precision:.2%}, Recall {recall:.2%}')
        accuracy, precision, recall = np.mean(self.test_results, axis=0)
        print(f'Average Test Results: Accuracy {accuracy:.2%}, 
            f'Precision {precision:.2%}, Recall {recall:.2%}')
\end{minted}


% -------------------------------------------
\section{Perceptron Model Implementation}\label{appendix:perceptron}

\begin{minted}[mathescape,
               linenos,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}

  class Perceptron:
     def __init__(self, weight_init='zeros', bias=True, max_epochs=500):
        self.max_epochs = max_epochs
        self.weight_init = weight_init
        self.bias = bias
        self.sgn = lambda z: np.where(z >= 0, 1, -1)

     def train(self, X_train, y_train):
        X, y = X_train.copy(), y_train.copy()
        n_samples, n_features = X.shape
        self.w = np.zeros(n_features) if self.weight_init == 'zeros'
            else np.random.randn(n_features) * 0.01
        self.b = 0
        epoch = 0

        while True:
            epoch += 1
            update = False

            if self.bias:
                for t in range(n_samples):
                    if y[t] * (np.dot(self.w, X[t]) + self.b) <= 0:
                        self.w += y[t] * X[t]
                        self.b += y[t]
                        update = True

            else:
                for t in range(n_samples):
                    if y[t] * np.dot(self.w, X[t]) <= 0:
                        self.w += y[t] * X[t]
                        update = True

            if (not update) or (epoch >= self.max_epochs):
                break

     def predict(self, X_test):
        return self.sgn(np.dot(X_test, self.w) + (self.b if self.bias else 0))

\end{minted}


% -------------------------------------------
\section{Pegasos SVM Model Implementation (hinge and logistic)}\label{appendix:pegasos}

\begin{minted}[mathescape,
               linenos,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}

  class PegasosSVM:
     def __init__(self, loss='hinge', T=1000, lambda_param=0.01):
        self.T = T
        self.lambda_param = lambda_param
        self.loss = loss
        self.sgn = lambda z: np.where(z >= 0, 1, -1)

     def _hinge(self, z_t):
        slack = self.y[z_t] * np.dot(self.w, self.X[z_t])
        return max(0, 1 - slack)

     def _logistic(self, z_t):
        z = -self.y[z_t] * np.dot(self.w, self.X[z_t])
        if z >= 0:
            return 1 / (1 + np.exp(-z))
        else:
            exp_z = np.exp(z)
            return exp_z / (1 + exp_z)

     def _gradient_loss(self, z_t):
        regularization_grad = self.lambda_param * self.w

        if self.loss == 'hinge':
            hinge_grad = -self.y[z_t] * self.X[z_t] * (self._hinge(z_t) > 0)
            return hinge_grad + regularization_grad

        elif self.loss == 'logistic':
            logistic_grad = -(self._logistic(z_t) / np.log(2))
                          * self.y[z_t] * self.X[z_t]
            return logistic_grad + regularization_grad

     def train(self, X_train, y_train):
        self.X, self.y = X_train.copy(), y_train.copy()
        n_samples, n_features = self.X.shape
        self.w = np.zeros(n_features)
        self.w_sum = np.zeros(n_features)
        if self.T > n_samples: self.T = n_samples

        for t in range(self.T):
            eta_t = 1 / (self.lambda_param * (t+1))
            z_t = np.random.randint(0, n_samples)
            self.w -= eta_t * self._gradient_loss(z_t)
            self.w_sum += self.w

        self.w = self.w_sum / self.T

     def predict(self, X_test):
        return self.sgn(np.dot(X_test, self.w))

\end{minted}


% -------------------------------------------
\section{Polynomial Second-degree Feature Expanded Nested Cross-Validation}\label{appendix:feature_expansion}

\begin{minted}[mathescape,
               linenos,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}

  class FeatureExpandedNestedCV(NestedCV):
     def __init__(self, dataset, model, param_grid, outer_K=5, inner_K=4):
        super().__init__(dataset, model, param_grid, outer_K, inner_K)

     def second_degree_feature_expansion(self):
        X_expanded = self.X.copy()

        for i in range(self.n_features):
            for j in range(i, self.n_features):
                new_feature = (X_expanded[:, i] * X_expanded[:, j]).reshape(-1, 1)
                X_expanded = np.hstack((X_expanded, new_feature))

        self.X = X_expanded
        self.n_features = self.X.shape[1]

\end{minted}


% -------------------------------------------
\section{Kernelized Perceptron}\label{appendix:kernel_perceptron}

\begin{minted}[mathescape,
               linenos,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}

  class KernelPerceptron:
     def __init__(self, kernel='polynomial', degree=3, gamma=1, max_epochs=10):
        self.kernel = kernel
        self.degree = degree
        self.gamma = gamma
        self.max_epochs = max_epochs
        self.sgn = lambda z: np.where(z >= 0, 1, -1)

     def _K(self, x1, x2):
        if self.kernel == 'polynomial':
            return (1 + np.dot(x1, x2)) ** self.degree
        elif self.kernel == 'gaussian':
            return np.exp(-(np.linalg.norm(x1 - x2) ** 2) / (2*self.gamma))

     def _kernelized_predict(self, X_t):
        return self.sgn(np.sum([self.y[s] * self._K(self.X[s], X_t)
               for s in self.S]))

     def train(self, X_train, y_train):
        self.X, self.y = X_train.copy(), y_train.copy()
        self.S = set()
        n_samples, n_features = self.X.shape
        epoch = 0

        while True:
            epoch += 1
            update = False

            for t in range(n_samples):
                y_hat = self._kernelized_predict(self.X[t])
                if self.y[t] != y_hat:
                    self.S.add(t)
                    update = True

            if (not update) or (epoch >= self.max_epochs):
                break

     def predict(self, X_test):
        return np.array([self._kernelized_predict(X) for X in X_test])

\end{minted}


% -------------------------------------------
\section{Kernelized Pegasos for SVM}\label{appendix:kernel_pegasos}

\begin{minted}[mathescape,
               linenos,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}

  class KernelPegasosSVM:
     def __init__(
            self,
            kernel='polynomial',
            degree=3,
            gamma=1,
            lambda_param=0.01,
            T=1000
        ):
        self.kernel = kernel
        self.degree = degree
        self.gamma = gamma
        self.lambda_param = lambda_param
        self.T = T
        self.sgn = lambda z: np.where(z >= 0, 1, -1)

     def _K(self, x1, x2):
            if self.kernel == 'polynomial':
                return (1 + np.dot(x1, x2)) ** self.degree
            elif self.kernel == 'gaussian':
                return np.exp(-(np.linalg.norm(x1-x2, axis=1) ** 2) / (2*self.gamma))

     def _kernelized_predict(self, X_zt):
        return np.sum(self.alpha * self.y * self._K(self.X, X_zt))

     def train(self, X_train, y_train):
        self.X, self.y = X_train.copy(), y_train.copy()
        n_samples, n_features = self.X.shape
        self.alpha = np.zeros(n_samples)
        if self.T > n_samples: self.T = n_samples

        for t in range(self.T):
            eta_t = 1 / (self.lambda_param * (t+1))
            z_t = np.random.randint(0, n_samples)
            if self.y[z_t] * eta_t * self._kernelized_predict(self.X[z_t]) < 1:
                self.alpha[z_t] += 1

     def predict(self, X_test):
        return np.array([self.sgn(self._kernelized_predict(X)) for X in X_test])

\end{minted}