
We know that $N = \Theta(d^n)$ is exponential in the degree $n$, and computing $\phi$ becomes practically impossible even for moderately large $n$. The computational complexity of using feature-expanded training sets for training linear predictors can be handled using \textit{kernels}, which helps us to find an efficiently computable kernel function $K: \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$ such that we have:
\begin{equation}
    K(x,x') = \phi(x)^T \phi(x') \quad \text{for all } x, x' \in \mathbb{R}^d
\end{equation}

The formulas for two of the most commonly used kernels, \textbf{polynomial} and \textbf{Gaussian} kernels, are provided below respectively:
\begin{equation}
    K_n(x,x') = (1 + x^T x')^n
\end{equation}
\begin{equation}
    K_\gamma(x,x') = \text{exp}\left(- \frac{\|x - x'\|^2}{2\gamma} \right)
\end{equation}

It should be noted that linear predictors in Reproducing Kernel Hilbert Spaces (RKHS) can suffer from overfitting. For a \textit{polynomial} kernel, increasing the degree $n$ reduces training error since higher-degree curves can better separate training points with complex decision boundaries, leading to overfitting when $n$ is too large. For \textit{Gaussian} kernels, if the $\gamma$ parameter (corresponding to the width of the Gaussians) is too small with respect to the distance $\|x - x'\|^2$ between
training and test points, the resulting predictor converges to 1-NN, leading to a training error equal or close to zero since they are never misclassified. In the following, we will attempt to implement:
\begin{enumerate}
    \item \textbf{Kernelized Perceptron} with \textit{Gaussian} and \textit{polynomial} kernels
    \item \textbf{Kernelized Pegasos for SVM} with \textit{Gaussian} and \textit{polynomial} kernels
\end{enumerate}


% ------------------------------------------------------
\section{Kernelized Perceptron}

Given a kernel $K$, the linear classifier generated by the Perceptron can be written as:
\begin{equation}
    h_K(x) = \text{sgn} \left(\sum_{s \in S} y_s K(x_s, x) \right)
\end{equation}

We will implement the kernelized Perceptron according to the following pseudo-code:

\begin{algorithm}[H]
    \SetAlgoLined
    \DontPrintSemicolon
    \caption{Kernel Perceptron}
    Let $S$ be the empty set, i.e. $S \leftarrow \emptyset$ \\
    \For{$t = 1, 2, \dots$}{
        Get the next example $(\boldsymbol{x}_t, y_t)$ \\
        Compute $\hat{y_t} = \text{sgn}\left(\sum\limits_{s \in S}\ y_s K(\boldsymbol{x}_s, \boldsymbol{x}_t) \right)$ \\
        \If{$\hat{y_t} \neq y_t$}{
            $S \leftarrow S \cup \{t\}$
        }
    }
\end{algorithm}

The implementation of the \texttt{KernelPerceptron} module is provided at the Appendix~\ref{appendix:kernel_perceptron}. Using the \texttt{kernel} parameter, we can choose either polynomial or Gaussian kernels to be used in the computation, where \texttt{degree} is used to define the polynomial degree and \texttt{gamma} is utilized as the input for Gaussian kernel. \texttt{max\_epochs} define the maximum epochs for the learning process, terminating if the convergence was not achieved upon this threshold. Following are the experiments performed on the kernelized Perceptron module.

\subsection{Polynomial Kernel}

Running for the polynomial kernel with \texttt{param\_grid = \{'kernel': ['polynomial'], 'degree': [2, 3]\}} for the \texttt{NestedCV} module, the hyper-parameter tuning was performed, where the \texttt{'polynomial'} kernel was explicitly fixed for evaluation. \texttt{degree = 2} was the best hyper-parameter combination selected by the folds on the validation set, which confirms that the second-degree polynomial had the best performance on this dataset. In a further step, using the best hyper-parameters in each fold the training set was re-trained and the model was evaluated on the test set accordingly. The whole hyper-parameter optimization process took \texttt{2h 2min 10s} to be completed.

The training set showed an average accuracy of \texttt{94.69\%}, with a precision of \texttt{94.50\%} and a recall of \texttt{94.55\%}. For the test set, the average accuracy was \texttt{94.46\%}, precision \texttt{94.36\%}, and recall \texttt{94.20\%}, which confirms the effectiveness of using polynomial kernel Perceptron along our dataset.

\subsection{Gaussian Kernel}

For the Gaussian kernel, hyper-parameter tuning was conducted using \texttt{param\_grid = \{'kernel': ['gaussian'], 'gamma': [0.01, 0.1, 1]\}} within the \texttt{NestedCV} module, with the \texttt{'gaussian'} kernel explicitly set for evaluation. The folds on the validation set identified \texttt{gamma = 1} as the optimal hyper-parameter. The model was re-trained on each fold's training set using the best hyper-parameters and then evaluated on the test set. The entire hyper-parameter optimization process took \texttt{9h 16min 31s} to complete.

The training set achieved an average accuracy of \texttt{99.03\%}, with a precision of \texttt{99.08\%} and a recall of \texttt{98.91\%}. On the test set, the average accuracy was \texttt{94.10\%}, with a precision of \texttt{94.07\%} and a recall of \texttt{93.72\%}, validating the success of applying the Gaussian kernel Perceptron to this dataset.


% ------------------------------------------------------
\section{Kernelized Pegasos for SVM}

One of the key advantages of SVMs is that they can be utilized with kernels rather than relying on direct access to the feature vectors $x$, and be represented as a linear combination of the training instances. To achieve this, instead of considering predictors as linear functions of the training examples, we consider them as linear functions of some implicit mapping $\phi(x)$. The minimization problem then becomes:
\begin{equation}
    \underset{\boldsymbol{w} \in \mathbb{R}^d}{\min} \ \underbrace{\frac{\lambda}{2} \Vert \boldsymbol{w} \Vert^2 + \frac{1}{m} \sum_{t = 1}^m \ell(w; (\phi(x_t),y_t))}_{F(\boldsymbol{w})}
\end{equation}
where
\begin{equation}
    \ell(w; (\phi(x_t),y_t)) = \max\{0, 1-y_t \langle w, \phi(x_t) \rangle \}
\end{equation}

and considering that the mapping $\phi(\cdot)$ is implicitly applied through a kernel function $K(\mathbf{x}, \mathbf{x'}) = \langle \phi(\mathbf{x}), \phi(\mathbf{x'}) \rangle$, thereby producing the inner products after the transformation by $\phi(\cdot)$.

We implement the kernelized Pegasos for SVM according to the pseudo-code provided at their official paper~\cite{shalev2007pegasos}, which is provided below:

\begin{algorithm}[H]
    \SetAlgoLined
    \DontPrintSemicolon
    \caption{Kernelized Pegasos for SVM} \vspace{5pt}
    \KwIn{Training set $(x_1, y_1), \dots, (x_m, y_m) \in \mathbb{R}^d \times \{-1, 1\}$}
    \text{Let $T$ be number of rounds, $\lambda > 0$ regularization coefficient} \\
    \textbf{Initialize } $\alpha_1 = (0, \dots, 0)$ \\
    \For{$t = 1, 2, \dots, T$}{
        Draw uniformly at random $Z_t \sim \{1, \dots, m\}$, obtaining $(x_{Z_t}, y_{Z_t})$ from training set \\
        \For{\textbf{all } $j \neq Z_t$}{
            Set $\alpha_{t+1}[j] = \alpha_t[j]$
        }
        \uIf{$y_{Z_t} \frac{1}{\lambda t} \sum_j \alpha_t[j] y_j K(x_{Z_t}, x_j) < 1$}{
            Set $\alpha_{t+1}[Z_t] = \alpha_t[Z_t] + 1$
        }
        \Else{
            Set $\alpha_{t+1}[Z_t] = \alpha_t[Z_t]$
        }
    }
    \KwOut{$\alpha_{T+1}$}
\end{algorithm}

The \texttt{KernelPegasosSVM} module is implemented and detailed in Appendix~\ref{appendix:kernel_pegasos}. Similar to the module implemented for \texttt{KernelPerceptron}, this class also requires \texttt{kernel}, \texttt{degree}, and \texttt{gamma} as parameters. Additionally, we have \texttt{lambda\_param} as the regularization coefficient, and \texttt{T} defining the number of rounds is replaced for the \texttt{max\_epochs} in the \texttt{KernelPerceptron} module. The following presents the experiments conducted using the kernelized Pegasos for SVM.

\subsection{Polynomial Kernel}

Using \texttt{NestedCV} module with \texttt{param\_grid = \{'kernel': ['polynomial'], 'degree': [2, 3], 'lambda\_param': [0.01, 0.1], 'T': [1000, 2000]\}}, the hyper-parameter tuning for the polynomial kernel was carried out, where the \texttt{'polynomial'} kernel was fixed for evaluation. Most of the folds on the validation set determined that \texttt{\{'degree': 2, 'lambda\_param': 0.1, 'T': 2000\}} was the best-performing hyper-parameter combination, indicating that the second-degree polynomial achieved the highest performance on this dataset. Subsequently, the model was re-trained on the training set of each fold using these optimal hyper-parameters and then evaluated on the test set. The entire hyper-parameter optimization process was completed in \texttt{2min 51s}.

The training set produced an average accuracy of \texttt{90.89\%}, with a precision of \texttt{91.03\%} and a recall of \texttt{90.16\%}. For the test set, the average accuracy was \texttt{90.32\%}, with a precision of \texttt{90.22\%} and a recall of \texttt{89.80\%}, confirming the effectiveness of applying the polynomial kernel Pegasos for SVM to this dataset.

\subsection{Gaussian Kernel}

For the Gaussian kernel, hyper-parameter tuning was performed using \texttt{param\_grid = \{'kernel': ['gaussian'], 'gamma': [0.01, 0.1, 1], 'lambda\_param': [1e-2, 1e-1], 'T': [1000, 2000]\}} within the \texttt{NestedCV} module, with the \texttt{'gaussian'} kernel fixed for evaluation. The model was then re-trained on the training set of each fold using these best hyper-parameters found and subsequently evaluated on the test set. The entire hyper-parameter optimization process took \texttt{7min 24s} to complete.

The training set achieved an average accuracy of \texttt{85.91\%}, with a precision of \texttt{91.11\%} and a recall of \texttt{78.58\%}. On the test set, the average accuracy was \texttt{84.87\%}, with a precision of \texttt{89.84\%} and a recall of \texttt{77.58\%}, validating the success of applying the Gaussian kernel Perceptron to this dataset. The results are not as good as the ones obtained from kernelized Perceptron, yet they are still promising.