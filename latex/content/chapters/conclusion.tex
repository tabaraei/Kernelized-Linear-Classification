
Throughout this project, we demonstrated the implementation and evaluation of various baseline machine learning algorithms—Perceptron, Pegasos SVM, and regularized logistic classification—for label classification based on numerical features. In addition, the feature-expanded and kernelized versions of such algorithms were investigated to analyze their performance. The following conclusions were drawn from the project.

{\bf Conclusion I:} It was observed that the baseline models could achieve limited performance due to their linear nature, particularly on non-linearly separable data. However, by employing polynomial second-degree feature-expansion and kernel methods, significant improvements and a remarkable increase of up to 25\% in accuracy were obtained. Specifically, the Polynomial Kernel Perceptron was identified as the best-performing model, surpassing all others in accuracy, precision, and recall.

{\bf Conclusion II:} The implementation of the Kernel Perceptron module faced a substantial computational cost, particularly Gaussian Kernel Perceptron requiring over \texttt{9} hours for hyper-parameter optimization, and polynomial Kernel Perceptron taking over \texttt{2} hours for completion. The analysis further revealed that implementation choices significantly impact runtime performance, as the inefficiency in the original kernel Perceptron implementation was identified and discussed, with recommendations for optimization.

{\bf Conclusion III:} In most cases, the difference between the training error and test error was negligible. This observation suggests that overfitting was not an issue, as none of the models performed exceptionally well on the training set while failing on the test set. However, mild underfitting was observed in the baseline models. This can be due to the fact that these models are linear classifiers, which are inherently limited in their ability to identify a separating hyperplane in non-linearly separable data.